# Unisimon Portal Scraper / LMS Agent Scraper

Un scraper automatizado para extraer tareas pendientes del portal de Unisimon Aula Pregrado.

**VersiÃ³n 2 (LMS Agent Scraper):** Framework generalizado con LangGraph, perfiles YAML, MCP y soporte para mÃºltiples portales LMS. Ver secciÃ³n "Uso LMS Agent Scraper (v2)" mÃ¡s abajo.

## ğŸš€ CaracterÃ­sticas

- âœ… AutenticaciÃ³n automÃ¡tica en el portal (Playwright)
- ğŸ“… Filtrado de tareas por perÃ­odo personalizable (dÃ­as adelante/atrÃ¡s)
- ğŸ“Š GeneraciÃ³n de reportes en formato Markdown
- ğŸ” Modo debug para anÃ¡lisis del portal
- ğŸ› ï¸ **v2:** Workflow LangGraph (auth â†’ discovery â†’ extracciÃ³n â†’ reporte), perfiles YAML, MCP
- ğŸ› ï¸ **v2:** DetecciÃ³n de cursos con LLM (Ollama), selectores Playwright/BeautifulSoup y fallback por contenido (visitar enlaces y clasificar con LLM)
- ğŸ› ï¸ Scripts legacy: `scraper.py`, `scraper_selenium.py`, `scraper_hybrid.py` para uso sin el paquete v2

## ğŸ“ Estructura del Proyecto

```
unisimon_scraper/
â”œâ”€â”€ scraper.py              # Script principal (legacy)
â”œâ”€â”€ scraper_selenium.py     # Alternativa con Selenium (legacy)
â”œâ”€â”€ scraper_hybrid.py       # HÃ­brido (legacy)
â”œâ”€â”€ config.py               # ConfiguraciÃ³n legacy
â”œâ”€â”€ utils.py                # Utilidades legacy
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml          # Paquete instalable (v2)
â”œâ”€â”€ .env.example            # Plantilla de variables para v2
â”œâ”€â”€ profiles/               # Perfiles YAML por portal (v2)
â”‚   â”œâ”€â”€ moodle_unisimon.yml
â”‚   â”œâ”€â”€ moodle_default.yml
â”‚   â””â”€â”€ ...
â”œâ”€â”€ src/lms_agent_scraper/  # LMS Agent Scraper (v2)
â”‚   â”œâ”€â”€ cli.py              # Comandos: run, profiles list/validate
â”‚   â”œâ”€â”€ agents/             # Agentes (login, course discovery, analyzer)
â”‚   â”œâ”€â”€ graph/              # Workflow LangGraph (nodes, workflow, state)
â”‚   â”œâ”€â”€ llm/                # Cliente Ollama (extracciÃ³n y clasificaciÃ³n)
â”‚   â”œâ”€â”€ tools/              # browser_tools, extraction_tools, report_tools
â”‚   â”œâ”€â”€ core/               # date_parser, profile_loader
â”‚   â””â”€â”€ mcp/                # Servidor MCP
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ AGENT_SKILLS.md
â”‚   â”œâ”€â”€ ARCHITECTURE_VERIFICATION.md
â”‚   â””â”€â”€ SOLID_AND_QUALITY.md
â”œâ”€â”€ tests/
â””â”€â”€ reports/                # Reportes Markdown generados
```

## âš™ï¸ ConfiguraciÃ³n

### 1. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 2. Configurar Credenciales

Edita `config.py` y actualiza las siguientes constantes:

```python
# Credenciales de acceso
USERNAME = 'tu_usuario'
PASSWORD = 'tu_contraseÃ±a'

# PerÃ­odo de consulta (en dÃ­as)
DAYS_AHEAD = 21  # Cambia este valor para modificar el perÃ­odo
```

### 3. Personalizar ConfiguraciÃ³n

Puedes modificar estas constantes en `config.py`:

- `DAYS_AHEAD`: NÃºmero de dÃ­as hacia adelante para buscar tareas (por defecto: 21 dÃ­as)
- `REQUEST_TIMEOUT`: Timeout para las peticiones HTTP (por defecto: 30 segundos)
- `DEBUG_MODE`: Activar/desactivar mensajes de debug
- `SAVE_HTML_DEBUG`: Guardar pÃ¡ginas HTML para anÃ¡lisis

## ğŸ¯ Uso

### EjecuciÃ³n BÃ¡sica

```bash
python scraper.py
```

### Salida

El script generarÃ¡:

1. **Reporte Markdown**: `reports/assignments_report_YYYYMMDD_HHMMSS.md`
2. **Archivos de Debug** (si estÃ¡ habilitado): `debug_html/`
3. **Mensajes en consola** con el progreso y resultados

## ğŸ“Š Formato del Reporte

El reporte incluye:

- ğŸ“… Fecha de generaciÃ³n y perÃ­odo consultado
- ğŸ“– Tareas agrupadas por curso
- â° Fechas de entrega con indicadores de urgencia
- ğŸ“ Descripciones y requisitos de cada tarea
- ğŸ”¢ Conteo total de tareas encontradas

## ğŸ”§ SoluciÃ³n de Problemas

### Error de AutenticaciÃ³n

Si el login falla:

1. Verifica las credenciales en `config.py`
2. Comprueba que la URL del portal sea correcta
3. Revisa los archivos HTML de debug en `debug_html/`

### No se Encuentran Tareas

Si no se encuentran tareas:

1. **Portal con JavaScript**: El portal podrÃ­a usar JavaScript para cargar contenido dinÃ¡micamente
2. **Estructura cambiada**: El portal podrÃ­a haber cambiado su estructura HTML
3. **Sin tareas**: Realmente no hay tareas en el perÃ­odo consultado

### MigraciÃ³n a Selenium

Si BeautifulSoup no es suficiente (contenido JavaScript), sigue estos pasos:

#### 1. Instalar Selenium

```bash
pip install selenium
```

#### 2. Instalar Driver del Navegador

**Para Chrome:**
```bash
# Descargar ChromeDriver desde: https://chromedriver.chromium.org/
# O usar webdriver-manager:
pip install webdriver-manager
```

#### 3. Crear `scraper_selenium.py`

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

class UnisimonSeleniumScraper:
    def __init__(self):
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service)
        self.driver.implicitly_wait(10)
    
    def login(self):
        self.driver.get(LOGIN_URL)
        # Implementar login con Selenium
        # ...
    
    def get_assignments(self):
        # Implementar extracciÃ³n con Selenium
        # ...
```

## ğŸ“ Notas Importantes

- âš ï¸ **Uso Responsable**: Este scraper es para uso personal Ãºnicamente
- ğŸ”’ **Seguridad**: En el flujo legacy las credenciales estÃ¡n en `config.py`; en v2 se usan variables de entorno (`.env`, no versionado; ver `.env.example`). Entorno virtual: `ENV_README.md`.
- ğŸ“Š **Limitaciones**: Depende de la estructura HTML del portal
- ğŸ”„ **Mantenimiento**: Puede requerir actualizaciones si el portal cambia

## ğŸ†˜ Soporte

Si encuentras problemas:

1. Revisa los archivos de debug en `debug_html/`
2. Verifica que las dependencias estÃ©n instaladas correctamente
3. Considera usar Selenium para portales con JavaScript
4. Revisa la estructura HTML del portal para actualizar los selectores

---

## LMS Agent Scraper (v2)

### Requisitos

- Python 3.10+
- Playwright: `pip install playwright && playwright install chromium`
- Variables de entorno en `.env` (copiar desde `.env.example`)

### ConfiguraciÃ³n

1. Copiar `.env.example` a `.env` y configurar:
   - `PORTAL_PROFILE` (ej: `moodle_unisimon` o `moodle_default`)
   - `PORTAL_BASE_URL`, `PORTAL_USERNAME`, `PORTAL_PASSWORD`
   - Opcional: `SCRAPER_DAYS_AHEAD`, `SCRAPER_DAYS_BEHIND`, `SCRAPER_MAX_COURSES`, `SCRAPER_OUTPUT_DIR`
   - Opcional (Ollama): `OLLAMA_BASE_URL`, `OLLAMA_MODEL_NAME`, `OLLAMA_TEMPERATURE`, `OLLAMA_NUM_CTX`, `OLLAMA_NUM_PREDICT` â€” usado para extraer la lista de cursos desde el HTML, clasificar pÃ¡ginas como â€œcursoâ€ en el discovery por contenido y (en el futuro) sugerir selectores. Requiere Ollama en ejecuciÃ³n y un modelo (p. ej. `ollama run glm-4.7-flash`). Ver [ollama.com/library/glm-4.7-flash](https://ollama.com/library/glm-4.7-flash). Si no estÃ¡ disponible, se usan Playwright y BeautifulSoup como respaldo.

2. Perfiles YAML en `profiles/` definen selectores, auth y opciones por portal (Moodle, Canvas, etc.). El perfil `moodle_unisimon` incluye `course_discovery` para el fallback por contenido.

### DetecciÃ³n de cursos (v2)

En la pÃ¡gina "Mis cursos" del portal, la lista de cursos se obtiene en este orden:

1. **LLM (Ollama)** â€” mÃ©todo principal: si Ollama estÃ¡ disponible, se envÃ­a un fragmento del HTML al modelo configurado (p. ej. GLM-4.7-Flash) para que devuelva un JSON con la lista de cursos (nombre y URL). Requiere Ollama en ejecuciÃ³n y el modelo descargado.
2. **Playwright** â€” respaldo: si el LLM no estÃ¡ disponible o no devuelve cursos, se espera a las tarjetas (p. ej. `[data-region='course-content']` o `.course-card`) y se extraen enlaces con los locators del perfil.
3. **BeautifulSoup** â€” respaldo: si aÃºn no hay cursos, se parsea el HTML y se buscan tarjetas, enlaces con clase `coursename` y URLs a `course/view.php`.
4. **Discovery por contenido** â€” fallback opcional (perfil `course_discovery.fallback_when_empty: true`): si sigue habiendo 0 cursos, se extraen enlaces candidatos de la pÃ¡gina, se visita cada uno con la misma sesiÃ³n y el LLM clasifica si el contenido es una pÃ¡gina de curso; solo esas URLs se consideran cursos. Configurable en el perfil (`max_candidates`, `candidate_patterns`). Ver perfil `moodle_unisimon.yml`.

### Uso

```bash
# Instalar el paquete en modo editable (desde la raÃ­z del repo)
pip install -e .

# Ejecutar scraper
python -m lms_agent_scraper.cli run

# Con perfil especÃ­fico (ej. Unisimon Aula Pregrado)
python -m lms_agent_scraper.cli run --profile moodle_unisimon

# Listar y validar perfiles
python -m lms_agent_scraper.cli profiles list
python -m lms_agent_scraper.cli profiles validate moodle_unisimon

# Tests (desde la raÃ­z del repo)
pytest tests/ -v
```

### MCP Server (Cursor / Claude Desktop)

En la configuraciÃ³n MCP del cliente:

```json
{
  "mcpServers": {
    "lms-scraper": {
      "command": "python",
      "args": ["-m", "lms_agent_scraper.mcp.server"],
      "env": {
        "PORTAL_PROFILE": "moodle_default",
        "PORTAL_BASE_URL": "${env:PORTAL_BASE_URL}",
        "PORTAL_USERNAME": "${env:PORTAL_USERNAME}",
        "PORTAL_PASSWORD": "${env:PORTAL_PASSWORD}"
      }
    }
  }
}
```

Herramientas expuestas: `get_pending_assignments`, `get_submitted_assignments`, `get_courses`, `generate_report`, `check_deadlines`, `list_profiles`. El servidor envÃ­a `instructions` al cliente indicando que se trata del portal LMS de la **Universidad SimÃ³n BolÃ­var (Unisimon), Aula Pregrado**, para que el agente tenga ese contexto.

### Desarrollo con Cursor

Este proyecto se desarrolla con Cursor. Se usan **reglas globales** en `~/.cursor/rules/` (aplican a todos los proyectos):

- **SOLID y calidad**: `solid-and-quality.mdc` â€” principios SOLID, DRY y buenas prÃ¡cticas. Copia en el repo: [docs/SOLID_AND_QUALITY.md](docs/SOLID_AND_QUALITY.md). VerificaciÃ³n de cumplimiento: [docs/ARCHITECTURE_VERIFICATION.md](docs/ARCHITECTURE_VERIFICATION.md).
- **Idioma**: `comments-spanish-code-english.mdc` â€” comentarios y docstrings en espaÃ±ol; nombres de cÃ³digo en inglÃ©s.

**MCPs opcionales** (instalados en `C:\MCPs\` y configurados en `~/.cursor/mcp.json`):

| MCP | DescripciÃ³n |
|-----|-------------|
| **pdf-reader** | Lee texto de archivos PDF. |
| **char-counter** | Cuenta caracteres, palabras y lÃ­neas en texto o archivo. |
| **unused-vars** | Analiza cÃ³digo Python y detecta variables/funciones no usadas y variables de entorno referenciadas; opcionalmente compara con un `.env`. |

Para analizar este repo con unused-vars (y opcionalmente el `.env`), el agente puede llamar a la herramienta `analyze_unused` con `path` al directorio del proyecto y `env_file` al `.env`.

### Agent Skills (skills.sh)

Para mejorar el comportamiento de agentes al trabajar con este repo, instala skills recomendados:

```bash
npx skills add vercel-labs/agent-browser
npx skills add anthropics/pdf
```

Ver `docs/AGENT_SKILLS.md` para la lista completa.

---

## ğŸ“„ Licencia

Este proyecto es para uso educativo y personal Ãºnicamente.
