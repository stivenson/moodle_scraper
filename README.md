# Unisimon Portal Scraper / LMS Agent Scraper

Un scraper automatizado para extraer tareas pendientes del portal de Unisimon Aula Pregrado.

**Versi√≥n 2 (LMS Agent Scraper):** Framework generalizado con LangGraph, perfiles YAML, MCP y soporte para m√∫ltiples portales LMS. Ver secci√≥n "Uso LMS Agent Scraper (v2)" m√°s abajo.

## üöÄ Caracter√≠sticas

- ‚úÖ Autenticaci√≥n autom√°tica en el portal
- üìÖ Filtrado de tareas por per√≠odo personalizable (configurable en d√≠as)
- üìä Generaci√≥n de reportes en formato Markdown
- üîç Modo debug para an√°lisis del portal
- üõ†Ô∏è Preparado para migraci√≥n a Selenium si es necesario

## üìÅ Estructura del Proyecto

```
unisimon_scraper/
‚îú‚îÄ‚îÄ scraper.py          # Script principal
‚îú‚îÄ‚îÄ config.py           # Configuraci√≥n y credenciales
‚îú‚îÄ‚îÄ utils.py            # Funciones auxiliares
‚îú‚îÄ‚îÄ requirements.txt    # Dependencias
‚îî‚îÄ‚îÄ README.md          # Este archivo
```

## ‚öôÔ∏è Configuraci√≥n

### 1. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 2. Configurar Credenciales

Edita `config.py` y actualiza las siguientes constantes:

```python
# Credenciales de acceso
USERNAME = 'tu_usuario'
PASSWORD = 'tu_contrase√±a'

# Per√≠odo de consulta (en d√≠as)
DAYS_AHEAD = 21  # Cambia este valor para modificar el per√≠odo
```

### 3. Personalizar Configuraci√≥n

Puedes modificar estas constantes en `config.py`:

- `DAYS_AHEAD`: N√∫mero de d√≠as hacia adelante para buscar tareas (por defecto: 21 d√≠as)
- `REQUEST_TIMEOUT`: Timeout para las peticiones HTTP (por defecto: 30 segundos)
- `DEBUG_MODE`: Activar/desactivar mensajes de debug
- `SAVE_HTML_DEBUG`: Guardar p√°ginas HTML para an√°lisis

## üéØ Uso

### Ejecuci√≥n B√°sica

```bash
python scraper.py
```

### Salida

El script generar√°:

1. **Reporte Markdown**: `reports/assignments_report_YYYYMMDD_HHMMSS.md`
2. **Archivos de Debug** (si est√° habilitado): `debug_html/`
3. **Mensajes en consola** con el progreso y resultados

## üìä Formato del Reporte

El reporte incluye:

- üìÖ Fecha de generaci√≥n y per√≠odo consultado
- üìñ Tareas agrupadas por curso
- ‚è∞ Fechas de entrega con indicadores de urgencia
- üìù Descripciones y requisitos de cada tarea
- üî¢ Conteo total de tareas encontradas

## üîß Soluci√≥n de Problemas

### Error de Autenticaci√≥n

Si el login falla:

1. Verifica las credenciales en `config.py`
2. Comprueba que la URL del portal sea correcta
3. Revisa los archivos HTML de debug en `debug_html/`

### No se Encuentran Tareas

Si no se encuentran tareas:

1. **Portal con JavaScript**: El portal podr√≠a usar JavaScript para cargar contenido din√°micamente
2. **Estructura cambiada**: El portal podr√≠a haber cambiado su estructura HTML
3. **Sin tareas**: Realmente no hay tareas en el per√≠odo consultado

### Migraci√≥n a Selenium

Si BeautifulSoup no es suficiente (contenido JavaScript), sigue estos pasos:

#### 1. Instalar Selenium

```bash
pip install selenium
```

#### 2. Instalar Driver del Navegador

**Para Chrome:**
```bash
# Descargar ChromeDriver desde: https://chromedriver.chromium.org/
# O usar webdriver-manager:
pip install webdriver-manager
```

#### 3. Crear `scraper_selenium.py`

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

class UnisimonSeleniumScraper:
    def __init__(self):
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service)
        self.driver.implicitly_wait(10)
    
    def login(self):
        self.driver.get(LOGIN_URL)
        # Implementar login con Selenium
        # ...
    
    def get_assignments(self):
        # Implementar extracci√≥n con Selenium
        # ...
```

## üìù Notas Importantes

- ‚ö†Ô∏è **Uso Responsable**: Este scraper es para uso personal √∫nicamente
- üîí **Seguridad**: Las credenciales se almacenan en texto plano en `config.py`
- üìä **Limitaciones**: Depende de la estructura HTML del portal
- üîÑ **Mantenimiento**: Puede requerir actualizaciones si el portal cambia

## üÜò Soporte

Si encuentras problemas:

1. Revisa los archivos de debug en `debug_html/`
2. Verifica que las dependencias est√©n instaladas correctamente
3. Considera usar Selenium para portales con JavaScript
4. Revisa la estructura HTML del portal para actualizar los selectores

---

## LMS Agent Scraper (v2)

### Requisitos

- Python 3.10+
- Playwright: `pip install playwright && playwright install chromium`
- Variables de entorno en `.env` (copiar desde `.env.example`)

### Configuraci√≥n

1. Copiar `.env.example` a `.env` y configurar:
   - `PORTAL_PROFILE` (ej: `moodle_default` o `moodle_unisimon`)
   - `PORTAL_BASE_URL`, `PORTAL_USERNAME`, `PORTAL_PASSWORD`
   - Opcional: `SCRAPER_DAYS_AHEAD`, `SCRAPER_DAYS_BEHIND`
   - Opcional (Ollama): `OLLAMA_BASE_URL`, `OLLAMA_MODEL_NAME`, `OLLAMA_TEMPERATURE`, `OLLAMA_NUM_CTX`, `OLLAMA_NUM_PREDICT`, `OLLAMA_REQUEST_TIMEOUT` ‚Äî m√©todo principal para extraer la lista de cursos cuando Ollama est√° disponible; tambi√©n para el agente analizador de selectores. Si Ollama no est√° disponible, se usan Playwright y BeautifulSoup como respaldo. Requiere Ollama en ejecuci√≥n y un modelo (p. ej. `ollama run glm-4.7-flash`). Ver [ollama.com/library/glm-4.7-flash](https://ollama.com/library/glm-4.7-flash).

2. Perfiles YAML en `profiles/` definen selectores por tipo de portal (Moodle, Canvas, etc.).

### Detecci√≥n de cursos (v2)

En la p√°gina "Mis cursos" del portal, la lista de cursos se obtiene as√≠:

1. **LLM (Ollama)** ‚Äî m√©todo principal: si Ollama est√° disponible, se env√≠a un fragmento del HTML al modelo configurado (p. ej. GLM-4.7-Flash) para que devuelva un JSON con la lista de cursos (nombre y URL). Requiere Ollama en ejecuci√≥n y el modelo descargado.
2. **Playwright** ‚Äî respaldo: si el LLM no est√° disponible o no devuelve cursos, se espera a las tarjetas (p. ej. `[data-region='course-content']` o `.course-card`) y se extraen enlaces con los locators del perfil.
3. **BeautifulSoup** ‚Äî respaldo: si a√∫n no hay cursos, se parsea el HTML y se buscan tarjetas, enlaces con clase `coursename` y URLs a `course/view.php`.

### Uso

```bash
# Instalar el paquete en modo editable (desde la ra√≠z del repo)
pip install -e .

# Ejecutar scraper
python -m lms_agent_scraper.cli run

# Con perfil espec√≠fico
python -m lms_agent_scraper.cli run --profile moodle_default

# Listar y validar perfiles
python -m lms_agent_scraper.cli profiles list
python -m lms_agent_scraper.cli profiles validate moodle_default
```

### MCP Server (Cursor / Claude Desktop)

En la configuraci√≥n MCP del cliente:

```json
{
  "mcpServers": {
    "lms-scraper": {
      "command": "python",
      "args": ["-m", "lms_agent_scraper.mcp.server"],
      "env": {
        "PORTAL_PROFILE": "moodle_default",
        "PORTAL_BASE_URL": "${env:PORTAL_BASE_URL}",
        "PORTAL_USERNAME": "${env:PORTAL_USERNAME}",
        "PORTAL_PASSWORD": "${env:PORTAL_PASSWORD}"
      }
    }
  }
}
```

Herramientas expuestas: `get_pending_assignments`, `get_submitted_assignments`, `get_courses`, `generate_report`, `check_deadlines`, `list_profiles`. El servidor env√≠a `instructions` al cliente indicando que se trata del portal LMS de la **Universidad Sim√≥n Bol√≠var (Unisimon), Aula Pregrado**, para que el agente tenga ese contexto.

### Desarrollo con Cursor

Este proyecto se desarrolla con Cursor. Se usan **reglas globales** en `~/.cursor/rules/` (aplican a todos los proyectos):

- **SOLID y calidad**: `solid-and-quality.mdc` ‚Äî principios SOLID, DRY y buenas pr√°cticas.
- **Idioma**: `comments-spanish-code-english.mdc` ‚Äî comentarios y docstrings en espa√±ol; nombres de c√≥digo en ingl√©s.

**MCPs opcionales** (instalados en `C:\MCPs\` y configurados en `~/.cursor/mcp.json`):

| MCP | Descripci√≥n |
|-----|-------------|
| **pdf-reader** | Lee texto de archivos PDF. |
| **char-counter** | Cuenta caracteres, palabras y l√≠neas en texto o archivo. |
| **unused-vars** | Analiza c√≥digo Python y detecta variables/funciones no usadas y variables de entorno referenciadas; opcionalmente compara con un `.env`. |

Para analizar este repo con unused-vars (y opcionalmente el `.env`), el agente puede llamar a la herramienta `analyze_unused` con `path` al directorio del proyecto y `env_file` al `.env`.

### Agent Skills (skills.sh)

Para mejorar el comportamiento de agentes al trabajar con este repo, instala skills recomendados:

```bash
npx skills add vercel-labs/agent-browser
npx skills add anthropics/pdf
```

Ver `docs/AGENT_SKILLS.md` para la lista completa.

---

## üìÑ Licencia

Este proyecto es para uso educativo y personal √∫nicamente.
